{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее  задание  № 1.  \n",
    "*Графематический и морфологический анализ текста*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:**\n",
    "\n",
    "Вычислить и проанализировать статистику морфологической омонимии в РЯ, взяв для этого 2-3 текста разных стилей/жанров\n",
    "и составив программу, которая на базе выбранного, программно подключаемого морфопроцессора определяет:\n",
    "\n",
    "– общее количество словоупотреблений, число различных словоформ, количество уникальных лемм, число незнакомых слов,\n",
    "коэффициент лексического богатства текста (= отношение числа различных лемм к общему числу словоупотреблений);\n",
    "\n",
    "–  абсолютную и относительную (с учетом/без учета неизменяемых слов) частоту омонимичных словоформ, абсолютную и\n",
    "относительную частоту словоформ с лексико-морфологической омонимией, максимальное и среднее число омонимов у \n",
    "словоформ текста, словоформы с наибольшим числом омонимов, наиболее частотный омоним\n",
    "\n",
    "В качестве подключаемого морфопроцессора можно взять, например:\n",
    "  mystem(http://api.yandex.ru/mystem/downloads/) или АОТ (http://www.aot.ru )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения анализа были выбраны 3 текста разных жанров (научный, художественный и публицистический). Тексты приведены в файле texts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В качестве подключаемого морфопроцессора я выбрала Mystem и pymorphy2 (https://pymorphy2.readthedocs.org/en/latest/#). \n",
    "\n",
    "В первом случае выбор обоснован интересом к продукции Яндекса, второй - более широким функционалом. Поэтому часть заданий реализовано с помощью Mystem, а другая часть с помощью Pymorph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка данных:**\n",
    "Прежде чем начинать анализировать данные, нужно их \"почистить\", привести в удобный для работы вид. Изначально мы имеем тесты со знаками препинания, цифрами, пробелами и другими символами. Нас интересуют лишь слова, поэтому была написана функция *make_clear_text*. Рассмотрим ее работу подробнее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# подключение необходимых библиотек\n",
    "import pymorphy2\n",
    "from texts import *\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_clear_text(text):\n",
    "    # текст без спец символов/цифр/пунктуации\n",
    "    m = Mystem()\n",
    "    clear_text = []\n",
    "    # переводим все заглавные буквы в строчные\n",
    "    lm = m.analyze(text.lower())\n",
    "    for i in range(0, len(lm)):\n",
    "        # в этом месте мы отбрасываем все посторонные символы, остается массив слов\n",
    "        if 'analysis' in lm[i]:\n",
    "            clear_text.append(lm[i]['text'])\n",
    "    return clear_text\n",
    "\n",
    "def no_one_symbol_text(text):\n",
    "    # убираем слова состоящие из одного символа\n",
    "    for i in text:\n",
    "        if len(i) == 1:\n",
    "            text.remove(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для написания этой функции был использован морфологический процессор Mystem. Он анализирует все элементы текста, расставляя тег \"analysis\" только словам, игнорируя все остальные символы. Этим я и воспользовалась. Хотя изначально была идея использовать регулярные выражения. \n",
    "Теперь все готово для проведения анализа, начнем с **общего количества совоупотреблений**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usage_count(text):\n",
    "    # общее количество словоупотреблений\n",
    "    # приводим данные в удобный вид, считаем, сколько слов в нашем тексте\n",
    "    result = len(make_clear_text(text))\n",
    "    print(\"Общее количество словоупотреблений: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализована функция, которая считает **количество различных словоформ**. Для этого необходимо сделать множество словоупотреблений, таким образом мы уберем повторяющиеся элементы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_different_word_forms(text):\n",
    "    # число различных словоформ\n",
    "    # приводим данные в удобный вид, получаем список всех слов в тексте\n",
    "    clear_text = make_clear_text(text)\n",
    "    # находим множество всех слов в тексте (теперь нет повторяющихся слов) и посчитаем их количество\n",
    "    result = len(set(clear_text))\n",
    "    print(\"Число различных словоформ: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию подсчитывающую **количество уникальных лемм**. Здесь я использовала морфопроцессор Mystem чтобы найти леммы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_unique_lemmas(text):\n",
    "    # количество уникальных лемм\n",
    "    m = Mystem()\n",
    "    # текст с леммами\n",
    "    lemmas = m.lemmatize(text)\n",
    "    # делим текст на слова\n",
    "    lemmatized_text = ''.join(lemmas)\n",
    "    # чистим данные от лишних символов\n",
    "    clear_text = no_one_symbol_text(make_clear_text(lemmatized_text))\n",
    "    result = len(set(clear_text))\n",
    "    print(\"Количество уникальных лемм: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим пунктом найдем **количество неизвестных слов**. У морфопроцессора **pymorph** есть очень удобная функция **word_is_known(word)**, она возвращает **True**, если слово известно, иначе - **False**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_unknown_words(text):\n",
    "    # число незнакомых слов\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    # суммируем количество неизвестных слов\n",
    "    result = sum([not morph.word_is_known(x) for x in clear_text])\n",
    "    print(\"Число незнакомых слов: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим метод вычисления коэффициента **лексического богатства текста**. Здесь очень просто, потому что мы уже умеем считать количество уникальных лемм и общее количество словоупотреблений. Осталось только разделить их друг на друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_richness_of_the_text_index(text):\n",
    "    # коэффициент лексического богатства текста\n",
    "    result = number_of_unique_lemmas(text)/usage_count(text)\n",
    "    print(\"Коэффициент лексического богатства текста:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию **абсолютной и относительной частоты омонимичных словоформ** без учета неизменяемых слов. Здесь нам нужно посчитать все слова у которых парсер выдает более одного типа разбора + надо учесть и отбросить те омонимы, у которых лексемы одинаковые. Для этого мы берем очередной омоним и склоняем все его разборы, чтобы найти лексемы. Далее находим множества этих лексем, и если оказывается, что в таких множествах 1 элемент - такой омоним неизменяемый, мы его отбрасываем. Но такой способ не всегда срабатывает. Существуют сокращениях слов (например \"руб\" - рубль), \"руб\" является неизменяемым омонимом, но когда мы начнем его склонять, морфопроцессор посчитает, что это \"рубль\", и склонять мы будем уже совсем не то, что хотели. Поэтому будем использовать встроенный в морфопроцессор тег m.tag если в нем есть параметр Fixd, то это неизменяемое слово. Опытным путем подтвердилось, что такой тег гарантирует работу для сокращений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_homonymous_word_forms(text):\n",
    "    # абсолютная и относительная (без учета/с учетом неизменяемых слов) частота омонимичных словоформ\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = 0\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    for j in clear_text:\n",
    "        m = morph.parse(j)\n",
    "        for k in range(0, len(m)):\n",
    "            lexeme = []\n",
    "            unchangeable_tag = []\n",
    "            unchangeable_tag.append('Fixd' in m[k].tag)\n",
    "            unchangeable = []\n",
    "            lex = m[k].lexeme\n",
    "            for i in lex:\n",
    "                lexeme.append(i.word)\n",
    "            lexeme = set(lexeme)\n",
    "            unchangeable.append(len(lexeme) == 1)\n",
    "        unchangeable = unchangeable + unchangeable_tag\n",
    "        if len(m) > 1 and True not in unchangeable:\n",
    "            result += 1\n",
    "    print(\"Абсолютная частота омонимичных словоформ без учета неизменяемых слов: \", result)\n",
    "    print(\"Относительная частота омонимичных словоформ без учета неизменяемых слов: \", result/len(clear_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция **абсолютной и относительной частоты омонимичных словоформ** c учетом неизменяемых слов. С учетом неизменяемых слов все эти действия просто не выполняем, нас интересуют как неизменяемые так и изменяемые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_homonymous_word_forms_full(text):\n",
    "    # абсолютная и относительная (с учетом неизменяемых слов) частота омонимичных словоформ\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = 0\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    for j in clear_text:\n",
    "        m = morph.parse(j)\n",
    "        # set_of_normal_forms = set([m[i].normal_form for i in range(0, len(m))])\n",
    "        # print(set_of_normal_forms)\n",
    "        if len(m) > 1:\n",
    "            result += 1\n",
    "    print(\"Абсолютная частота омонимичных словоформ с учетом неизменяемых слов: \", result)\n",
    "    print(\"Относительная частота омонимичных словоформ с учетом неизменяемых слов: \", result/len(clear_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_of_word_forms_with_lexicalmorphological_homonymy(text):\n",
    "    # абсолютная и относительнаю частота словоформ с лексико-морфологической омонимией\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = 0\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    set_of_lexem = []\n",
    "    for j in clear_text:\n",
    "        m = morph.parse(j)\n",
    "        if morph.word_is_known(j):\n",
    "            set_of_lexem = set([m[i][4][0][2] for i in range(0, len(m))])\n",
    "        if len(m) > 1 and len(set_of_lexem) > 1:\n",
    "            result += 1\n",
    "    print(\"Абсолютная частота словоформ с лексико-морфологической омонимией: \", result)\n",
    "    print(\"Относительная частота словоформ с лексико-морфологической омонимией: \", result/len(clear_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим функцию, которая находит **максимальное и среднее число омонимов у словоформ текста, словоформу с наибольшим числом омонимов, наиболее частотный омоним**.\n",
    "\n",
    "Для того чтобы найти максимальное число омонимов нужно запоминать текущее число омонимов у очередного слова и сравнимать с предыдущим максимальным числом, если текущее больше чем предыдущее, сделаем его максимальным и так далее. Перебрав таким образом все слова, надем максимум.\n",
    "\n",
    "Чтобы найти среднее нужно просто суммировать количество омонимов каждого слова потом разделить на их количество.\n",
    "\n",
    "Словоформа с наибольшим числом омонимов находится как и максимальное число омонимов: мы просто каждый раз запоминаем слово вместе с числом.\n",
    "\n",
    "С наиболее частотным омонимом дела обстоят сложнее, тут нужно хранить словарик, у которого ключи это слова, а значения ключей - число, означающее сколько раз нам такое слово встретилось. Если слово нам еще не встречалось, мы добавляем в словарь новый ключ, если слово оказалось омонимом, то значение этого ключа увеличивается на единицу. В конце обработки текста находим наибольшее значение и показываем как результат его ключ те самый популярый омоним. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximum_and_average_number_of_homonyms_in_the_text_word_forms(text):\n",
    "    # максимальное и среднее число омонимов у словоформ текста, словоформа с наибольшим числом омонимов,\n",
    "    # наиболее частотный омоним\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = {}\n",
    "    max_homonym = 0\n",
    "    mean_homonym = 0\n",
    "    counter = 0\n",
    "    clear_text = no_one_symbol_text(make_clear_text(text))\n",
    "    for j in clear_text:\n",
    "        set_of_lexem = []\n",
    "        if j not in result:\n",
    "            result[j] = 0\n",
    "        m = morph.parse(j)\n",
    "        if len(m) > 1:\n",
    "            result[j] += 1\n",
    "            counter += 1\n",
    "            mean_homonym += len(m)\n",
    "            if len(m) > max_homonym:\n",
    "                max_homonym = len(m)\n",
    "                wordform_with_max_homonym = j\n",
    "    print(\"Максимальное число омонимов у словоформ: \", max_homonym)\n",
    "    print(\"Среднее число омонимов у словоформ: \", mean_homonym/counter)\n",
    "    print(\"Словоформа с наибольшим числом омонимов: \", wordform_with_max_homonym)\n",
    "    print(\"Наиболее частотный омоним: \",  max(result, key=result.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ текстов:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим все написанные функции к текстам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число различных словоформ:  78\n",
      "Количество уникальных лемм:  71\n",
      "Общее количество словоупотреблений:  84\n",
      "Коэффициент лексического богатства текста: 0.8452380952380952\n",
      "Число незнакомых слов:  2\n",
      "Абсолютная частота омонимичных словоформ с учетом неизменяемых слов:  51\n",
      "Относительная частота омонимичных словоформ с учетом неизменяемых слов:  0.6710526315789473\n",
      "Абсолютная частота омонимичных словоформ без учета неизменяемых слов:  44\n",
      "Относительная частота омонимичных словоформ без учета неизменяемых слов:  0.5789473684210527\n",
      "Абсолютная частота словоформ с лексико-морфологической омонимией:  20\n",
      "Относительная частота словоформ с лексико-морфологической омонимией:  0.2631578947368421\n",
      "Максимальное число омонимов у словоформ:  26\n",
      "Среднее число омонимов у словоформ:  3.736842105263158\n",
      "Словоформа с наибольшим числом омонимов:  их\n",
      "Наиболее частотный омоним:  на\n"
     ]
    }
   ],
   "source": [
    "count_different_word_forms(scientific_text)\n",
    "lexical_richness_of_the_text_index(scientific_text)\n",
    "number_of_unknown_words(scientific_text)\n",
    "frequency_homonymous_word_forms_full(scientific_text)\n",
    "frequency_homonymous_word_forms(scientific_text)\n",
    "frequency_of_word_forms_with_lexicalmorphological_homonymy(scientific_text)\n",
    "maximum_and_average_number_of_homonyms_in_the_text_word_forms(scientific_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Художественный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число различных словоформ:  304\n",
      "Количество уникальных лемм:  262\n",
      "Общее количество словоупотреблений:  397\n",
      "Коэффициент лексического богатства текста: 0.6599496221662469\n",
      "Число незнакомых слов:  10\n",
      "Абсолютная частота омонимичных словоформ с учетом неизменяемых слов:  174\n",
      "Относительная частота омонимичных словоформ с учетом неизменяемых слов:  0.5838926174496645\n",
      "Абсолютная частота омонимичных словоформ без учета неизменяемых слов:  147\n",
      "Относительная частота омонимичных словоформ без учета неизменяемых слов:  0.49328859060402686\n",
      "Абсолютная частота словоформ с лексико-морфологической омонимией:  72\n",
      "Относительная частота словоформ с лексико-морфологической омонимией:  0.24161073825503357\n",
      "Максимальное число омонимов у словоформ:  28\n",
      "Среднее число омонимов у словоформ:  3.365740740740741\n",
      "Словоформа с наибольшим числом омонимов:  его\n",
      "Наиболее частотный омоним:  новые\n"
     ]
    }
   ],
   "source": [
    "count_different_word_forms(artistic_text)\n",
    "lexical_richness_of_the_text_index(artistic_text)\n",
    "number_of_unknown_words(artistic_text)\n",
    "frequency_homonymous_word_forms_full(artistic_text)\n",
    "frequency_homonymous_word_forms(artistic_text)\n",
    "frequency_of_word_forms_with_lexicalmorphological_homonymy(artistic_text)\n",
    "maximum_and_average_number_of_homonyms_in_the_text_word_forms(artistic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Публицистический текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число различных словоформ:  304\n",
      "Количество уникальных лемм:  262\n",
      "Общее количество словоупотреблений:  397\n",
      "Коэффициент лексического богатства текста: 0.6599496221662469\n",
      "Число незнакомых слов:  10\n",
      "Абсолютная частота омонимичных словоформ с учетом неизменяемых слов:  174\n",
      "Относительная частота омонимичных словоформ с учетом неизменяемых слов:  0.5838926174496645\n",
      "Абсолютная частота омонимичных словоформ без учета неизменяемых слов:  147\n",
      "Относительная частота омонимичных словоформ без учета неизменяемых слов:  0.49328859060402686\n",
      "Абсолютная частота словоформ с лексико-морфологической омонимией:  73\n",
      "Относительная частота словоформ с лексико-морфологической омонимией:  0.24496644295302014\n",
      "Максимальное число омонимов у словоформ:  28\n",
      "Среднее число омонимов у словоформ:  3.365740740740741\n",
      "Словоформа с наибольшим числом омонимов:  его\n",
      "Наиболее частотный омоним:  новые\n"
     ]
    }
   ],
   "source": [
    "count_different_word_forms(artistic_text)\n",
    "lexical_richness_of_the_text_index(artistic_text)\n",
    "number_of_unknown_words(artistic_text)\n",
    "frequency_homonymous_word_forms_full(artistic_text)\n",
    "frequency_homonymous_word_forms(artistic_text)\n",
    "frequency_of_word_forms_with_lexicalmorphological_homonymy(artistic_text)\n",
    "maximum_and_average_number_of_homonyms_in_the_text_word_forms(artistic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
