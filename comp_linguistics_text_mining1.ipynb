{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее  задание  № 1.  \n",
    "*Графематический и морфологический анализ текста*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:**\n",
    "\n",
    "Вычислить и проанализировать статистику морфологической омонимии в РЯ, взяв для этого 2-3 текста разных стилей/жанров\n",
    "и составив программу, которая на базе выбранного, программно подключаемого морфопроцессора определяет:\n",
    "\n",
    "– общее количество словоупотреблений, число различных словоформ, количество уникальных лемм, число незнакомых слов,\n",
    "коэффициент лексического богатства текста (= отношение числа различных лемм к общему числу словоупотреблений);\n",
    "\n",
    "–  абсолютную и относительную (с учетом/без учета неизменяемых слов) частоту омонимичных словоформ, абсолютную и\n",
    "относительную частоту словоформ с лексико-морфологической омонимией, максимальное и среднее число омонимов у \n",
    "словоформ текста, словоформы с наибольшим числом омонимов, наиболее частотный омоним\n",
    "\n",
    "В качестве подключаемого морфопроцессора можно взять, например:\n",
    "  mystem(http://api.yandex.ru/mystem/downloads/) или АОТ (http://www.aot.ru )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения анализа были выбраны 3 текста разных жанров (научный, художественный и публицистический). Тексты приведены в файле texts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В качестве подключаемого морфопроцессора я выбрала Mystem и pymorphy2 (https://pymorphy2.readthedocs.org/en/latest/#). \n",
    "\n",
    "В первом случае выбор обоснован интересом к продукции Яндекса, второй - более широким функционалом. Поэтому часть заданий реализовано с помощью Mystem, а другая часть с помощью Pymorph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка данных:**\n",
    "Прежде чем начинать анализировать данные, нужно их \"почистить\", привести в удобный для работы вид. Изначально мы имеем тесты со знаками препинания, цифрами, пробелами и другими символами. Нас интересуют лишь слова, поэтому была написана функция *make_clear_text*. Рассмотрим ее работу подробнее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# подключение необходимых библиотек\n",
    "import pymorphy2\n",
    "from texts import *\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_clear_text(text):\n",
    "    # текст без спец символов/цифр/пунктуации\n",
    "    m = Mystem()\n",
    "    clear_text = []\n",
    "    # переводим все заглавные буквы в строчные\n",
    "    lm = m.analyze(text.lower())\n",
    "    for i in range(0, len(lm)):\n",
    "        # в этом месте мы отбрасываем все посторонные символы, остается массив слов\n",
    "        if 'analysis' in lm[i]:\n",
    "            clear_text.append(lm[i]['text'])\n",
    "    return clear_text\n",
    "\n",
    "def no_one_symbol_text(text):\n",
    "    # убираем слова состоящие из одного символа\n",
    "    for i in text:\n",
    "        if len(i) == 1:\n",
    "            text.remove(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для написания этой функции был использован морфологический процессор Mystem. Он анализирует все элементы текста, расставляя тег \"analysis\" только словам, игнорируя все остальные символы. Этим я и воспользовалась. Хотя изначально была идея использовать регулярные выражения. \n",
    "Теперь все готово для проведения анализа, начнем с **общего количества совоупотреблений**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usage_count(text):\n",
    "    # общее количество словоупотреблений\n",
    "    # приводим данные в удобный вид, считаем, сколько слов в нашем тексте\n",
    "    result = len(make_clear_text(text))\n",
    "    print(\"Общее количество словоупотреблений: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_different_word_forms(text):\n",
    "    # число различных словоформ\n",
    "    # приводим данные в удобный вид, получаем список всех слов в тексте\n",
    "    clear_text = make_clear_text(text)\n",
    "    # находим множество всех слов в тексте (теперь нет повторяющихся слов) и посчитаем их количество\n",
    "    result = len(set(clear_text))\n",
    "    print(\"Число различных словоформ: \", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
