{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее  задание  № 1.  \n",
    "*Графематический и морфологический анализ текста*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:**\n",
    "\n",
    "Вычислить и проанализировать статистику морфологической омонимии в РЯ, взяв для этого 2-3 текста разных стилей/жанров\n",
    "и составив программу, которая на базе выбранного, программно подключаемого морфопроцессора определяет:\n",
    "\n",
    "– общее количество словоупотреблений, число различных словоформ, количество уникальных лемм, число незнакомых слов,\n",
    "коэффициент лексического богатства текста (= отношение числа различных лемм к общему числу словоупотреблений);\n",
    "\n",
    "–  абсолютную и относительную (с учетом/без учета неизменяемых слов) частоту омонимичных словоформ, абсолютную и\n",
    "относительную частоту словоформ с лексико-морфологической омонимией, максимальное и среднее число омонимов у \n",
    "словоформ текста, словоформы с наибольшим числом омонимов, наиболее частотный омоним\n",
    "\n",
    "В качестве подключаемого морфопроцессора можно взять, например:\n",
    "  mystem(http://api.yandex.ru/mystem/downloads/) или АОТ (http://www.aot.ru )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения анализа были выбраны 3 текста разных жанров (научный, художественный и публицистический). Тексты приведены в файле texts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В качестве подключаемого морфопроцессора я выбрала Mystem и pymorphy2 (https://pymorphy2.readthedocs.org/en/latest/#). \n",
    "\n",
    "В первом случае выбор обоснован интересом к продукции Яндекса, второй - более широким функционалом. Поэтому часть заданий реализовано с помощью Mystem, а другая часть с помощью Pymorph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка данных:**\n",
    "Прежде чем начинать анализировать данные, нужно их \"почистить\", привести в удобный для работы вид. Изначально мы имеем тесты со знаками препинания, цифрами, пробелами и другими символами. Нас интересуют лишь слова, поэтому была написана функция *make_clear_text*. Рассмотрим ее работу подробнее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# подключение необходимых библиотек\n",
    "import pymorphy2\n",
    "from texts import *\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_clear_text(text):\n",
    "    # текст без спец символов/цифр/пунктуации\n",
    "    m = Mystem()\n",
    "    clear_text = []\n",
    "    # переводим все заглавные буквы в строчные\n",
    "    lm = m.analyze(text.lower())\n",
    "    for i in range(0, len(lm)):\n",
    "        # в этом месте мы отбрасываем все посторонные символы, остается массив слов\n",
    "        if 'analysis' in lm[i]:\n",
    "            clear_text.append(lm[i]['text'])\n",
    "    return clear_text\n",
    "\n",
    "def no_one_symbol_text(text):\n",
    "    # убираем слова состоящие из одного символа\n",
    "    for i in text:\n",
    "        if len(i) == 1:\n",
    "            text.remove(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для написания этой функции был использован морфологический процессор Mystem. Он анализирует все элементы текста, расставляя тег \"analysis\" только словам, игнорируя все остальные символы. Этим я и воспользовалась. Хотя изначально была идея использовать регулярные выражения. \n",
    "Теперь все готово для проведения анализа, начнем с **общего количества совоупотреблений**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usage_count(text):\n",
    "    # общее количество словоупотреблений\n",
    "    # приводим данные в удобный вид, считаем, сколько слов в нашем тексте\n",
    "    result = len(make_clear_text(text))\n",
    "    print(\"Общее количество словоупотреблений: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализована функция, которая считает **количество различных словоформ**. Для этого необходимо сделать множество словоупотреблений, таким образом мы уберем повторяющиеся элементы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_different_word_forms(text):\n",
    "    # число различных словоформ\n",
    "    # приводим данные в удобный вид, получаем список всех слов в тексте\n",
    "    clear_text = make_clear_text(text)\n",
    "    # находим множество всех слов в тексте (теперь нет повторяющихся слов) и посчитаем их количество\n",
    "    result = len(set(clear_text))\n",
    "    print(\"Число различных словоформ: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию подсчитывающую **количество уникальных лемм**. Здесь я использовала морфопроцессор Mystem чтобы найти леммы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_unique_lemmas(text):\n",
    "    # количество уникальных лемм\n",
    "    m = Mystem()\n",
    "    # текст с леммами\n",
    "    lemmas = m.lemmatize(text)\n",
    "    # делим текст на слова\n",
    "    lemmatized_text = ''.join(lemmas)\n",
    "    # чистим данные от лишних символов\n",
    "    clear_text = no_one_symbol_text(make_clear_text(lemmatized_text))\n",
    "    result = len(set(clear_text))\n",
    "    print(\"Количество уникальных лемм: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим пунктом найдем **количество неизвестных слов**. У морфопроцессора **pymorph** есть очень удобная функция **word_is_known(word)**, она возвращает **True**, если слово известно, иначе - **False**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_unknown_words(text):\n",
    "    # число незнакомых слов\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    # суммируем количество неизвестных слов\n",
    "    result = sum([not morph.word_is_known(x) for x in clear_text])\n",
    "    print(\"Число незнакомых слов: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим метод вычисления коэффициента **лексического богатства текста**. Здесь очень просто, потому что мы уже умеем считать количество уникальных лемм и общее количество словоупотреблений. Осталось только разделить их друг на друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_richness_of_the_text_index(text):\n",
    "    # коэффициент лексического богатства текста\n",
    "    result = number_of_unique_lemmas(text)/usage_count(text)\n",
    "    print(\"Коэффициент лексического богатства текста:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию **абсолютной и относительной частоты омонимичных словоформ** без учета неизменяемых слов. Здесь нам нужно посчитать все слова у которых парсер выдает более одного типа разбора + надо учесть и отбросить те омонимы, у которых лексемы одинаковые. Для этого мы берем очередной омоним и склоняем все его разборы, чтобы найти лексемы. Далее находим множества этих лексем, и если оказывается, что в таких множествах 1 элемент - такой омоним неизменяемый, мы его отбрасываем. Но такой способ не всегда срабатывает. Существуют сокращениях слов (например \"руб\" - рубль), \"руб\" является неизменяемым омонимом, но когда мы начнем его склонять, морфопроцессор посчитает, что это \"рубль\", и склонять мы будем уже совсем не то, что хотели. Поэтому будем использовать встроенный в морфопроцессор тег m.tag если в нем есть параметр Fixd, то это неизменяемое слово. Опытным путем подтвердилось, что такой тег гарантирует работу для сокращений.\n",
    "\n",
    "С учетом неизменяемых слов все эти действия просто не выполняем, нас интересуют как неизменяемые так и изменяемые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_homonymous_word_forms(text):\n",
    "    # абсолютная и относительная (без учета/с учетом неизменяемых слов) частота омонимичных словоформ\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = 0\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    for j in clear_text:\n",
    "        m = morph.parse(j)\n",
    "        for k in range(0, len(m)):\n",
    "            lexeme = []\n",
    "            unchangeable_tag = []\n",
    "            unchangeable_tag.append('Fixd' in m[k].tag)\n",
    "            unchangeable = []\n",
    "            lex = m[k].lexeme\n",
    "            for i in lex:\n",
    "                lexeme.append(i.word)\n",
    "            lexeme = set(lexeme)\n",
    "            unchangeable.append(len(lexeme) == 1)\n",
    "        unchangeable = unchangeable + unchangeable_tag\n",
    "        if len(m) > 1 and True not in unchangeable:\n",
    "            result += 1\n",
    "    print(\"Абсолютная частота омонимичных словоформ без учета неизменяемых слов: \", result)\n",
    "    print(\"Относительная частота омонимичных словоформ без учета неизменяемых слов: \", result/len(clear_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_of_word_forms_with_lexicalmorphological_homonymy(text):\n",
    "    # абсолютная и относительнаю частота словоформ с лексико-морфологической омонимией\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = 0\n",
    "    clear_text = set(no_one_symbol_text(make_clear_text(text)))\n",
    "    for j in clear_text:\n",
    "        m = morph.parse(j)\n",
    "        set_of_lexem = set([m[i][4][0][2] for i in range(0, len(m))])\n",
    "        if len(m) > 1 and len(set_of_lexem) != 1:\n",
    "            result += 1\n",
    "    print(\"Абсолютная частота словоформ с лексико-морфологической омонимией: \", result)\n",
    "    print(\"Относительная частота словоформ с лексико-морфологической омонимией: \", result/len(clear_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим функцию, которая находит **максимальное и среднее число омонимов у словоформ текста, словоформу с наибольшим числом омонимов, наиболее частотный омоним**.\n",
    "\n",
    "Для того чтобы найти максимальное число омонимов нужно запоминать текущее число омонимов у очередного слова и сравнимать с предыдущим максимальным числом, если текущее больше чем предыдущее, сделаем его максимальным и так далее. Перебрав таким образом все слова, надем максимум.\n",
    "\n",
    "Чтобы найти среднее нужно просто суммировать количество омонимов каждого слова потом разделить на их количество.\n",
    "\n",
    "Словоформа с наибольшим числом омонимов находится как и максимальное число омонимов: мы просто каждый раз запоминаем слово вместе с числом.\n",
    "\n",
    "С наиболее частотным омонимом дела обстоят сложнее, тут нужно хранить словарик, у которого ключи это слова, а значения ключей - число, означающее сколько раз нам такое слово встретилось. Если слово нам еще не встречалось, мы добавляем в словарь новый ключ, если слово оказалось омонимом то значение этого ключа увеличивается на единицу. В конце обработки текста находим наибольшее значение и показываем как результат его ключ те самый популярый омоним. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximum_and_average_number_of_homonyms_in_the_text_word_forms(text):\n",
    "    # максимальное и среднее число омонимов у словоформ текста\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    result = {}\n",
    "    max_homonym = 0\n",
    "    mean_homonym = 0\n",
    "    counter = 0\n",
    "    clear_text = no_one_symbol_text(make_clear_text(text))\n",
    "    for j in clear_text:\n",
    "        set_of_lexem = []\n",
    "        if j not in result:\n",
    "            result[j] = 0\n",
    "        m = morph.parse(j)\n",
    "        if len(m) > 1:\n",
    "            result[j] += 1\n",
    "            counter += 1\n",
    "            mean_homonym += len(m)\n",
    "            if len(m) > max_homonym:\n",
    "                max_homonym = len(m)\n",
    "                wordform_with_max_homonym = j\n",
    "    print(\"Максимальное число омонимов у словоформ: \", max_homonym)\n",
    "    print(\"Среднее число омонимов у словоформ: \", mean_homonym/counter)\n",
    "    print(\"Словоформа с наибольшим числом омонимов: \", wordform_with_max_homonym)\n",
    "    print(\"Наиболее частотный омоним: \",  max(result, key=result.get))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
